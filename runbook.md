# To do as followings

0. use VPN
1. create folder
   1. mnt/ollama-webui
   2. mnt/ollama
2. docker-compose up -d
3. load model
   1. lanch the CLI of ollama, and execute the following command: ollama run llama3
4. navigate to page: <http://localhost:3000/>

## References

<https://dev.to/timesurgelabs/how-to-run-llama-3-locally-with-ollama-and-open-webui-297d>